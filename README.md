# Second-order Optimization Methods in Training Deep Neural Networks

Optimization in machine learning, both theoretical and applied, is dominated by first-order gradient methods such as stochastic gradient descent (SGD). In this article, we explore the applicability of second-order methods to the image classification problem. We analyze the performance of the Convolutional Neural Network trained with AdaHesian, LBFGS and BB methods in comparison to the first-order optimization methods such as SGD, SGD-momentum and Adam.

# Repository structure


