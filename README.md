# Second-order Optimization Methods in Deep Neural Networks Training
This repository contains code and report for the final project of *Optimization for Machine learning (CS-439)* course.

## Abstract
Optimization in machine learning, both theoretical and applied, is dominated by first-order gradient methods such as stochastic gradient descent (SGD). In this article, we explore the applicability of second-order methods to the image classification problem. We analyze the performance of the Convolutional Neural Network trained with AdaHesian, LBFGS and BB methods in comparison to the first-order optimization methods such as SGD, SGD-momentum and Adam.

## Repository structure

## Team

This project is accomplished by:  
- Anastasia Filippova anastasia.filippova@epfl.ch
- Sofia Blinova sofia.blinova@epfl.ch
- Tikhon Parshokov tikhon.parshikov@epfl.ch
